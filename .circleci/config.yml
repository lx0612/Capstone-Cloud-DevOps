version: 2.1
# Use a package of configuration called an orb.
orbs:
  aws-cli: circleci/aws-cli@2.0.3
  python: circleci/python@2.1.1
  kubernetes: circleci/kubernetes@1.3.1
  aws-eks: circleci/aws-eks@2.2.0
commands:
  # Reusable Job Code
  export:
    steps: 
      - run:
          name: "Get git commit short-hash for the current"
          command: |
            # Git hash for the current commit
            echo 'export current_commit=<< pipeline.git.revision >>' >> "$BASH_ENV"
            echo 'export CUR_SHORT_HASH="${current_commit:0:7}"' >> "$BASH_ENV"
            # echo 'export CUR_SHORT_HASH="test"' >> "$BASH_ENV"
      - run:
          name: "Export environment variable"
          command: |
            echo 'export WORKSPACE_DIR="/home/circleci/workspace"' >> "$BASH_ENV"


  destroy_environment:
    parameters:
      stack_name:
        type: string
      when:
        default: "on_fail"
        type: enum
        enum: ["always", "on_success", "on_fail"]
      wait:
        description: Wait stack delete complete
        type: boolean
        default: false

    steps:     
      - run:
          name: "Delete << parameters.stack_name >> stack"
          when: << parameters.when >>
          shell: /bin/bash
          command: |
            # Get stack id for the delete_stack waiter
            stack_info=$(aws cloudformation describe-stacks --stack-name << parameters.stack_name >> --query "Stacks[*] | [0].StackId" 2>&1)
            if echo $stack_info | grep 'does not exist' > /dev/null
            then
              echo "Stack does not exist."
              echo $stack_info
              exit 0
            fi
            if echo $stack_info | grep 'ValidationError' > /dev/null
            then
              echo $stack_info
              exit 1
            else
              aws cloudformation delete-stack --stack-name << parameters.stack_name >>
              echo $stack_info

              if [ "<< parameters.wait >>" = true ]
              then
                echo "Wait stack delete complete..."
                aws cloudformation wait stack-delete-complete --stack-name << parameters.stack_name >>
              fi

              if [ "<< parameters.when >>" = "on_fail" ]
              then
                echo "Roll back completed. Green environment destroyed."
                exit 0
              fi
              echo "Stack << parameters.stack_name >> cleaned up"
              exit 0
            fi

# Define the jobs we want to run for this project
jobs:
  build-test-app:
    executor: 
      name: python/default
      tag: "3.7.7"

    environment:
      # Set the DOCKERHUB_USERNAME environment variable for pushing the image to Docker Hub
      DOCKERHUB_USERNAME: lx96
      IMAGE_NAME: flask_service
      IMAGE_TAG: latest

    steps:
      # This command will pull all of the files into a directory called project which can be found at ~/project.
      - checkout
      # Export environment variable
      - export
      # Set up Docker environment
      - setup_remote_docker

      # Download and cache dependencies
      - restore_cache:
          keys:
          - new-dependencies-{{ checksum "requirements.txt" }}
          # fallback to using the latest cache if no exact match is found
          - dependencies-

      - run:
          name: Install dependencies
          command: |
            ls
            pwd
            cd $(eval echo "$CIRCLE_WORKING_DIRECTORY")
            python3 -m venv venv
            . venv/bin/activate
            pip install --upgrade pip
            pip install -r requirements.txt

      - save_cache:
          paths:
            - /home/circleci/project/venv
          key: new-dependencies-{{ checksum "requirements.txt" }}

      - run:
          name: Run lint
          command: |
            cd $(eval echo "$CIRCLE_WORKING_DIRECTORY")
            . venv/bin/activate
            pylint main.py --disable=R,C,W1203,W1202,unsubscriptable-object,no-name-in-module  

      - run: 
          name: Build image
          command: |
            echo "$DOCKERHUB_PASSWORD" | docker login --username $DOCKERHUB_USERNAME --password-stdin
            docker build -t $IMAGE_NAME:$IMAGE_TAG .
      - run:
          name: Push to Docker Hub
          command: |
            # Log in to Docker Hub
            docker tag $IMAGE_NAME $DOCKERHUB_USERNAME/$IMAGE_NAME
            docker push $DOCKERHUB_USERNAME/$IMAGE_NAME:$IMAGE_TAG

  create-cluster-eks:
    docker:
      - image: cimg/python:3.7
    steps:
      # Export environment variable
      - export
      - kubernetes/install:
          kubectl-version: v1.22.0
      - aws-eks/create-cluster:
          cluster-name: cluster-app-$CUR_SHORT_HASH
          region: "us-east-1"
          zones: "us-east-1a,us-east-1b"
          node-type: "t3.medium"
          nodegroup-name: "node-eks"
          node-volume-size: 15
          nodes-max: 3
          nodes: 2
          nodes-min: 1
          tags: "Owner=lx96"
          show-eksctl-command: true

  test-cluster-eks:
    docker:
      - image: cimg/python:3.7
    steps:
      # Export environment variable
      - export
      - kubernetes/install:
          kubectl-version: v1.22.0
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: cluster-app-$CUR_SHORT_HASH
      - run:
          command: |
            kubectl get services
          name: Test cluster

  create-deployment-kubenetes:
    docker:
      - image: cimg/python:3.7
    steps:
      # This command will pull all of the files into a directory called project which can be found at ~/project.
      - checkout
      # Export environment variable
      - export
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: mlops-cluster-$CUR_SHORT_HASH
          install-kubectl: true
      - kubernetes/create-or-update-resource:
          get-rollout-status: true
          watch-rollout-status: true
          resource-file-path: deployment.yaml
          resource-name: deployment/app-deployment
          show-kubectl-command: true
      - run:
          name: Check status when deployment fail
          when: "on_fail"
          command: kubectl get events

  clean-up:
    executor: aws-cli/default
    steps:
      # This command will pull all of the files into a directory called project which can be found at ~/project.
      - checkout
      # Export environment variable
      - export
      # Set up AWS Credentials
      - aws-cli/setup
      - run:
          name: Create working directory
          command: |
            # no error if existing, make parent directories as needed
            mkdir --parents $(eval echo "$WORKSPACE_DIR")
      - run:
          name: "Fetch stacks and save the old stack name"
          command: |
            # Fetch the stack names
            export STACKS=(
              $(aws cloudformation list-stacks \
                  --query "StackSummaries[*].StackName" \
                  --no-paginate --output text \
                  --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE
              )
            )
            for stack in ${STACKS[@]}
            do
              if [[ ! "$stack" =~ "$CUR_SHORT_HASH" ]]
              then
                if [[ "$stack" =~ "cluster-app" ]] && [[ "$stack" =~ "nodegroup" ]]
                then
                  touch $WORKSPACE_DIR/destroy_cluster_nodegroup_stack
                  echo $stack > $WORKSPACE_DIR/destroy_cluster_nodegroup_stack
                  
                  echo "destroy_cluster_nodegroup_stack:" $stack
                fi
                if [[ "$stack" =~ "cluster-app" ]] && [[ ! "$stack" =~ "nodegroup" ]]
                then
                  touch $WORKSPACE_DIR/destroy_cluster_stack
                  echo $stack > $WORKSPACE_DIR/destroy_cluster_stack

                  echo "destroy_cluster_stack:" $stack
                fi
              fi
            done

      # Remove the cluster nodegroup infrastructure
      - destroy_environment:
          stack_name: $(eval cat $WORKSPACE_DIR/destroy_cluster_nodegroup_stack)
          when: "always"
          wait: true
          
      # Remove the cluster infrastructure
      - destroy_environment:
          stack_name: $(eval cat $WORKSPACE_DIR/destroy_cluster_stack)
          when: "always"

# Sequential workflow
workflows:
  my_workflow:
    jobs:
      - build-test-app
      - create-cluster-eks:
          requires:
            - "build-test-app"
          filters:
            branches:
              only:
                - main
      - test-cluster-eks:
          requires:
            - create-cluster-eks
          filters:
            branches:
              only:
                - main
      - create-deployment-kubenetes:
          requires:
            - test-cluster-eks
          filters:
            branches:
              only:
                - main
      - clean-up:
          requires:
            - create-deployment-kubenetes
          filters:
            branches:
              only:
                - main